{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertForSequenceClassification, get_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Check if the current `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and drop missing values\n",
    "df = pd.read_excel(\"train.xlsx\")\n",
    "df = df.dropna(subset=[\"comment_text\", \"cyberbullying\"])\n",
    "\n",
    "# Ensure types\n",
    "df[\"comment_text\"] = df[\"comment_text\"].astype(str)\n",
    "df[\"cyberbullying\"] = df[\"cyberbullying\"].astype(int)\n",
    "\n",
    "# Extra safety: remove non-stringy things\n",
    "def is_valid_text(t):\n",
    "    return isinstance(t, str) and len(t.strip()) > 0\n",
    "\n",
    "df = df[df[\"comment_text\"].apply(is_valid_text)]\n",
    "\n",
    "# Final clean list conversion\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"comment_text\"].tolist(),\n",
    "    df[\"cyberbullying\"].tolist(),\n",
    "    test_size=0.3, # 30% test size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split train into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    test_size=0.1,  # 10% of the original train set goes to validation\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize with attention masks and return tensors\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyberbullyingDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CyberbullyingDataset(train_encodings, train_labels)\n",
    "val_dataset = CyberbullyingDataset(val_encodings, val_labels)\n",
    "test_dataset = CyberbullyingDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(teacher_model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 6279/6279 [14:14<00:00,  7.35it/s, loss=0.0182]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 training loss: 0.0958\n",
      "Validation Accuracy: 0.9694\n",
      "Epoch 1 took 880.56 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 6279/6279 [14:12<00:00,  7.36it/s, loss=0.000743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 training loss: 0.0517\n",
      "Validation Accuracy: 0.9678\n",
      "Epoch 2 took 879.03 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 6279/6279 [14:11<00:00,  7.38it/s, loss=0.00163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 training loss: 0.0214\n",
      "Validation Accuracy: 0.9689\n",
      "Epoch 3 took 877.55 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    teacher_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    teacher_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} training loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} took {epoch_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_student(\n",
    "    teacher_model, student_model,\n",
    "    train_loader, val_loader,\n",
    "    optimizer, temperature=2.0, alpha=0.5, epochs=5\n",
    "):\n",
    "    loss_ce = nn.CrossEntropyLoss()  # For hard labels\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        teacher_model.eval()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Distill Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Teacher predictions\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits / temperature\n",
    "\n",
    "            # Student predictions\n",
    "            student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            student_logits = student_outputs.logits / temperature\n",
    "\n",
    "            # Distillation loss (KL + CE)\n",
    "            soft_loss = F.kl_div(\n",
    "                input=F.log_softmax(student_logits, dim=-1),\n",
    "                target=F.softmax(teacher_logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (temperature ** 2)\n",
    "\n",
    "            hard_loss = loss_ce(student_outputs.logits, labels)\n",
    "            loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1} distillation loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "        # Validation\n",
    "        student_model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                val_loss += loss_ce(logits, labels).item()\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    return student_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Distill Epoch 1/3: 100%|██████████| 6279/6279 [11:29<00:00,  9.11it/s, loss=0.00206] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 distillation loss: 0.0307\n",
      "Epoch time: 689.34s\n",
      "Validation Loss: 0.1390, Accuracy: 0.9667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distill Epoch 2/3: 100%|██████████| 6279/6279 [11:44<00:00,  8.91it/s, loss=9.7e-5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 distillation loss: 0.0233\n",
      "Epoch time: 704.33s\n",
      "Validation Loss: 0.1407, Accuracy: 0.9677\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distill Epoch 3/3: 100%|██████████| 6279/6279 [11:36<00:00,  9.02it/s, loss=0.0176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 distillation loss: 0.0208\n",
      "Epoch time: 696.38s\n",
      "Validation Loss: 0.1428, Accuracy: 0.9654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_optimizer = AdamW(student_model.parameters(), lr=2e-5)\n",
    "\n",
    "binary_classification_kd = distill_student(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=student_optimizer,\n",
    "    temperature=2.0,\n",
    "    alpha=0.5,\n",
    "    epochs=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_classification_kd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m joblib.dump(\u001b[43mbinary_classification_kd\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mbinary_classification_kd.joblib\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'binary_classification_kd' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(binary_classification_kd.state_dict(), 'models/binary_classification_kd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saran\\AppData\\Local\\Temp\\ipykernel_9436\\1988717321.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mj = torch.load('models/binary_classification_kd.joblib',map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mj = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/binary_classification_kd.joblib\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:1384\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1383\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:1628\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33mreadinto\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[32m3\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m0\u001b[39m) <= sys.version_info < (\u001b[32m3\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1623\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1624\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mReceived object of type \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunctionality.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1626\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1628\u001b[39m magic_number = \u001b[43mpickle_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic_number != MAGIC_NUMBER:\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid magic number; corrupt file?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\storage.py:520\u001b[39m, in \u001b[36m_load_from_bytes\u001b[39m\u001b[34m(b)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_from_bytes\u001b[39m(b):\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:1384\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1383\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:1638\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1636\u001b[39m unpickler = UnpicklerWrapper(f, **pickle_load_args)\n\u001b[32m   1637\u001b[39m unpickler.persistent_load = persistent_load\n\u001b[32m-> \u001b[39m\u001b[32m1638\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1640\u001b[39m deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n\u001b[32m   1642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._guards.active_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:1566\u001b[39m, in \u001b[36m_legacy_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1564\u001b[39m     obj = cast(Storage, torch.UntypedStorage(nbytes))\n\u001b[32m   1565\u001b[39m     obj._torch_load_uninitialized = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m     obj = \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   1568\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   1569\u001b[39m typed_storage = torch.storage.TypedStorage(\n\u001b[32m   1570\u001b[39m     wrap_storage=obj, dtype=dtype, _internal=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:601\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    583\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:539\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    537\u001b[39m     backend_name = torch._C._get_privateuse1_backend_name()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     device = \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\AI_env\\Lib\\site-packages\\torch\\serialization.py:508\u001b[39m, in \u001b[36m_validate_device\u001b[39m\u001b[34m(location, backend_name)\u001b[39m\n\u001b[32m    506\u001b[39m     device_index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mis_available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.is_available() is False. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mdevice_count\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    516\u001b[39m     device_count = device_module.device_count()\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "student_model.load_state_dict(torch.load('models/binary_classification_kd.pth'))\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "model.load_state_dict(torch.load(\"models/binary_classification_kd.pth\"))\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "\n",
    "text = [\"You're such a loser!\"]\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the same device\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Prediction:\", predictions.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m onnx_model_path = \u001b[33m\"\u001b[39m\u001b[33mmodels/binary_classification_kd.onnx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m dummy_input = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: torch.randint(\u001b[32m0\u001b[39m, \u001b[32m1000\u001b[39m, (\u001b[32m1\u001b[39m, \u001b[32m128\u001b[39m)).to(device),\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: torch.ones((\u001b[32m1\u001b[39m, \u001b[32m128\u001b[39m)).to(device)\n\u001b[32m      7\u001b[39m }\n\u001b[32m      9\u001b[39m torch.onnx.export(\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mstudent_model\u001b[49m,\n\u001b[32m     11\u001b[39m     (dummy_input[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m], dummy_input[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     12\u001b[39m     onnx_model_path,\n\u001b[32m     13\u001b[39m     input_names=[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     14\u001b[39m     output_names=[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     15\u001b[39m     dynamic_axes={\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m}},\n\u001b[32m     16\u001b[39m     opset_version=\u001b[32m14\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mONNX model exported to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'student_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Export the trained student model to ONNX format\n",
    "onnx_model_path = \"models/binary_classification_kd.onnx\"\n",
    "\n",
    "dummy_input = {\n",
    "    \"input_ids\": torch.randint(0, 1000, (1, 128)).to(device),\n",
    "    \"attention_mask\": torch.ones((1, 128)).to(device)\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    student_model,\n",
    "    (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}},\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "print(f\"ONNX model exported to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the ONNX model for optimized inference\n",
    "quantized_model_path = \"models/binary_classification_kd_quantized.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=onnx_model_path,\n",
    "    model_output=quantized_model_path,\n",
    "    weight_type=QuantType.QINT8\n",
    ")\n",
    "\n",
    "print(f\"Quantized ONNX model saved to {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the quantized ONNX model\n",
    "ort_session = ort.InferenceSession(quantized_model_path)\n",
    "\n",
    "text = [\"You're such a loser!\"]\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "onnx_inputs = {\n",
    "    \"input_ids\": inputs[\"input_ids\"].numpy(),\n",
    "    \"attention_mask\": inputs[\"attention_mask\"].numpy()\n",
    "}\n",
    "\n",
    "onnx_outputs = ort_session.run(None, onnx_inputs)\n",
    "predictions = torch.argmax(torch.tensor(onnx_outputs[0]), dim=1)\n",
    "\n",
    "print(\"ONNX Prediction:\", predictions.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
