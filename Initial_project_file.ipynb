{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85db4c5-1881-4027-85a8-ae420771f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install -U evaluate\n",
    "!pip install -U datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import accelerate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b24ea0-b580-4549-8a14-5cdc823d1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 11657/11657 [00:00<00:00, 1201390.81 examples/s]\n",
      "Generating train split: 100%|██████████| 41961/41961 [00:00<00:00, 1904504.77 examples/s]\n",
      "Generating validation split: 100%|██████████| 4663/4663 [00:00<00:00, 867973.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['tweet_id', 'text', 'label'],\n",
      "        num_rows: 11657\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tweet_id', 'text', 'label'],\n",
      "        num_rows: 41961\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tweet_id', 'text', 'label'],\n",
      "        num_rows: 4663\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"socialmediaie/SocialMediaIE-MetaCorpus-v1\", \"abusive-founta\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc37116",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = set(dataset['train']['label'])\n",
    "for val in dataset['train']['label']:\n",
    "    if val not in unique_values:\n",
    "        print(f\"Duplicate value found: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa32f7-d4bf-44e7-9fe0-c448f77dd184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezral\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ezral\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|██████████████████████████████████████████████████████████████| 11657/11657 [00:03<00:00, 3322.90 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████| 41961/41961 [00:12<00:00, 3454.28 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 4663/4663 [00:01<00:00, 3505.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 11657\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 41961\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4663\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Choose a BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # Or any other BERT variant\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Define a function to tokenize the text and encode labels\n",
    "# @parameter: examples: a dictionary with keys \"text\" and \"label\"\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128) #adjust max length as needed\n",
    "    # Map labels to numerical IDs\n",
    "    label_map = {\"normal\": 0, \"abusive\": 1, \"hateful\": 2, \"spam\": 3}  # Define your label mapping\n",
    "    labels = [label_map[label] for label in examples[\"label\"]]\n",
    "    tokenized_inputs[\"labels\"] = labels # Add labels to the dictionary\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 3. Apply the preprocessing function to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 4. Remove unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"tweet_id\", \"text\", \"label\"])\n",
    "\n",
    "# 5. Rename the label column to labels (required by some Transformers models)\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# 6. Set the format to PyTorch tensors (or TensorFlow, if you prefer)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78e1e5-dd02-4af8-a314-bdf55c70ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, hidden_dropout_prob=0.1)\n",
    "model.resize_token_embeddings(len(tokenizer)) # need to resize due to new tokens added\n",
    "\n",
    "# 2. Define training arguments\n",
    "metric_name = 'f1'\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"./snapshots/{model_name}-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit = 3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# 3. Define a metric to compute during training\n",
    "metric = evaluate.load(metric_name)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "\n",
    "# 4. Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 5. Train the model\n",
    "train_log = trainer.train()\n",
    "\n",
    "trainer.save_model(\"./models/myFinetunedModel\") # for saving your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4ec56-d43f-4a35-a94c-afde0c1966f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we perform our evaluation on our test set using the fine-tuned model from earlier.\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "results = classifier(dataset['test']['text'], max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "dfResults = pd.DataFrame.from_dict(results)\n",
    "dfResults['label'] = dfResults['label'].str.replace('LABEL_','')\n",
    "f1 = metric.compute(predictions=dfResults['label'].tolist(), references=dataset['test']['label'], average='micro')\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
